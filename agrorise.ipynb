{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPYSOpdeeAmI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle as pk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.models import load_model\n",
        "import tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ5HgSqPeFVE"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lixfEx0ebPW"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHBBmrsUhw4O"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "El1xf5XiejuC"
      },
      "outputs": [],
      "source": [
        "def trainIntentModel():\n",
        "    # Load the dataset and prepare it to the train the model\n",
        "\n",
        "    # Importing dataset and splitting into words and labels\n",
        "    dataset = pd.read_csv('/content/drive/MyDrive/intent.csv', names=[\"Query\", \"Intent\"])\n",
        "\n",
        "    X = dataset[\"Query\"]\n",
        "    y = dataset[\"Intent\"]\n",
        "\n",
        "    unique_intent_list = list(set(y))\n",
        "\n",
        "    print(\"Intent Dataset successfully loaded!\")\n",
        "\n",
        "    # Clean and prepare the intents corpus\n",
        "    queryCorpus = []\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for query in X:\n",
        "        query = re.sub('[^a-zA-Z]', ' ', query)\n",
        "\n",
        "        # Tokenize sentence\n",
        "        query = query.split(' ')\n",
        "\n",
        "        # Lemmatizing\n",
        "        tokenized_query = [ps.stem(word.lower()) for word in query]\n",
        "\n",
        "        # Recreate the sentence from tokens\n",
        "        tokenized_query = ' '.join(tokenized_query)\n",
        "\n",
        "        # Add to corpus\n",
        "        queryCorpus.append(tokenized_query)\n",
        "\n",
        "    print(queryCorpus)\n",
        "    print(\"Corpus created\")\n",
        "\n",
        "    countVectorizer= CountVectorizer(max_features=800)\n",
        "    corpus = countVectorizer.fit_transform(queryCorpus).toarray()\n",
        "    print(corpus.shape)\n",
        "    print(\"Bag of words created!\")\n",
        "\n",
        "    # Save the CountVectorizer\n",
        "    pk.dump(countVectorizer, open(\"/content/drive/MyDrive/IntentCountVectorizer.sav\", 'wb'))\n",
        "    print(\"Intent CountVectorizer saved!\")\n",
        "\n",
        "    # Encode the intent classes\n",
        "    labelencoder_intent = LabelEncoder()\n",
        "    y = labelencoder_intent.fit_transform(y)\n",
        "\n",
        "\n",
        "    y = to_categorical(y)\n",
        "    print(\"Encoded the intent classes!\")\n",
        "    print(y)\n",
        "\n",
        "    # Return a dictionary, mapping labels to their integer values\n",
        "    res = {}\n",
        "    for cl in labelencoder_intent.classes_:\n",
        "        res.update({cl:labelencoder_intent.transform([cl])[0]})\n",
        "\n",
        "    intent_label_map = res\n",
        "    print(intent_label_map)\n",
        "    print(\"Intent Label mapping obtained!\")\n",
        "\n",
        "    # Initialising the Aritifcial Neural Network\n",
        "    classifier = Sequential()\n",
        "\n",
        "    # Adding the input layer and the first hidden layer\n",
        "    classifier.add(Dense(units = 96, kernel_initializer = 'uniform', activation = 'relu', input_dim = 133))\n",
        "\n",
        "    # Adding the second hidden layer\n",
        "    classifier.add(Dense(units = 96, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "    # Adding the output layer\n",
        "    classifier.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'softmax'))\n",
        "\n",
        "    # Compiling the ANN\n",
        "    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "    # Fitting the ANN to the Training set\n",
        "    classifier.fit(corpus, y, batch_size = 10, epochs = 500)\n",
        "\n",
        "    return classifier, intent_label_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "intent_model, intent_label_map = trainIntentModel()\n",
        "\n",
        "# Save the Intent model\n",
        "intent_model.save('/content/drive/MyDrive/intent_model.h5')\n",
        "print(\"Intent model saved!\")\n",
        "\n",
        "\n",
        "\n",
        "def trainEntityModel():\n",
        "    # Importing dataset and splitting into words and labels\n",
        "    dataset = pd.read_csv('/content/drive/MyDrive/data-tags.csv', names=[\"word\", \"label\"])\n",
        "    X = dataset.iloc[:, :-1].values\n",
        "    y = dataset.iloc[:, 1].values\n",
        "#     X = X.reshape(630,)\n",
        "    print(X)\n",
        "    print(\"Entity Dataset successfully loaded!\")\n",
        "\n",
        "    entityCorpus=[]\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    # Stem words in X\n",
        "    for word in X.astype(str):\n",
        "        word = [ps.stem(word[0])]\n",
        "        entityCorpus.append(word)\n",
        "\n",
        "    print(entityCorpus)\n",
        "    X = entityCorpus\n",
        "    from numpy import array\n",
        "    X = array(X)\n",
        "    X = X.reshape(len(X),)\n",
        "\n",
        "    # Create a bag of words model for words\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    cv = CountVectorizer(max_features=1500)\n",
        "#     X = cv.fit_transform(X.astype('U')).toarray()\n",
        "    X = cv.fit_transform(X).toarray()\n",
        "    print(\"Entity Bag of words created!\")\n",
        "\n",
        "    # Save CountVectorizer state\n",
        "    pk.dump(cv, open('/content/drive/MyDrive/EntityCountVectorizer.sav', 'wb'))  #iuoij\n",
        "    print(\"Entity CountVectorizer state saved!\")\n",
        "\n",
        "    # Encoding categorical data of labels\n",
        "    labelencoder_y = LabelEncoder()\n",
        "    y = labelencoder_y.fit_transform(y.astype(str))\n",
        "    print(\"Encoded the entity classes!\")\n",
        "\n",
        "    # Return a dict mapping labels to their integer values\n",
        "    res = {}\n",
        "    for cl in labelencoder_y.classes_:\n",
        "        res.update({cl:labelencoder_y.transform([cl])[0]})\n",
        "    entity_label_map = res\n",
        "    print(\"Entity Label mapping obtained!\")\n",
        "\n",
        "    # Fit classifier to dataset\n",
        "    classifier = GaussianNB()\n",
        "    classifier.fit(X, y)\n",
        "    print(\"Entity Model trained successfully!\")\n",
        "\n",
        "    # Save the entity classifier model\n",
        "    pk.dump(classifier, open('/content/drive/MyDrive/entity_model.sav', 'wb'))\n",
        "    print(\"Trained entity model saved!\")\n",
        "\n",
        "    return entity_label_map\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load Entity model\n",
        "entity_label_map = trainEntityModel()\n",
        "\n",
        "loadedEntityCV = pk.load(open('/content/drive/MyDrive/EntityCountVectorizer.sav', 'rb'))\n",
        "loadedEntityClassifier = pk.load(open('/content/drive/MyDrive/entity_model.sav', 'rb'))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def getEntities(query):\n",
        "    query = loadedEntityCV.transform(query).toarray()\n",
        "\n",
        "    response_tags = loadedEntityClassifier.predict(query)\n",
        "\n",
        "    entity_list=[]\n",
        "    for tag in response_tags:\n",
        "        if tag in entity_label_map.values():\n",
        "            entity_list.append(list(entity_label_map.keys())[list(entity_label_map.values()).index(tag)])\n",
        "\n",
        "    return entity_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "with open('/content/drive/MyDrive/intents.json') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "# Load model to predict user result\n",
        "loadedIntentClassifier = load_model('/content/drive/MyDrive/intent_model.h5')\n",
        "loaded_intent_CV = pk.load(open('/content/drive/MyDrive/IntentCountVectorizer.sav', 'rb'))\n",
        "\n",
        "USER_INTENT = \"\"\n",
        "\n",
        "while True:\n",
        "    user_query = input()\n",
        "\n",
        "    query = re.sub('[^a-zA-Z]', ' ', user_query)\n",
        "\n",
        "    # Tokenize sentence\n",
        "    query = query.split(' ')\n",
        "\n",
        "    # Lemmatizing\n",
        "    ps = PorterStemmer()\n",
        "    tokenized_query = [ps.stem(word.lower()) for word in query]\n",
        "\n",
        "    # Recreate the sentence from tokens\n",
        "    processed_text = ' '.join(tokenized_query)\n",
        "\n",
        "    # Transform the query using the CountVectorizer\n",
        "    processed_text = loaded_intent_CV.transform([processed_text]).toarray()\n",
        "\n",
        "    # Make the prediction\n",
        "    predicted_Intent = loadedIntentClassifier.predict(processed_text)\n",
        "#     print(predicted_Intent)\n",
        "    result = np.argmax(predicted_Intent, axis=1)\n",
        "\n",
        "    for key, value in intent_label_map.items():\n",
        "        if value==result[0]:\n",
        "            #print(key)\n",
        "            USER_INTENT = key\n",
        "            break\n",
        "\n",
        "    for i in intents['intents']:\n",
        "        if i['tag'] == USER_INTENT:\n",
        "            print(random.choice(i['responses']))\n",
        "\n",
        "\n",
        "    # Extract entities from text\n",
        "    entities = getEntities(tokenized_query)\n",
        "\n",
        "    # Mapping between tokens and entity tags\n",
        "    token_entity_map = dict(zip(entities, tokenized_query))\n",
        "    # print(token_entity_map)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}